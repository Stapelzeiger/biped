{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ORIGINAL_DATA_PATH = \"../../sim_mujoco/data/\"\n",
    "EXPORT_DATA_PATH = \"./updated_data/\"\n",
    "ORIGINAL_NAME = \"no_controller_input\"\n",
    "EXPORT_VER = \"v4\"\n",
    "\n",
    "INPUT_FILE = ORIGINAL_DATA_PATH + ORIGINAL_NAME + \".csv\"\n",
    "OUTPUT_FILE = EXPORT_DATA_PATH + ORIGINAL_NAME + \"_\" + EXPORT_VER + \".csv\"\n",
    "\n",
    "\n",
    "data_rel_paths = [\n",
    "    # \"../../sim_mujoco/data/dataset_backwards.csv\", \"../../sim_mujoco/data/dataset_forward_sideways.csv\", \"../../sim_mujoco/data/dataset_misc.csv\"\n",
    "    # \"../../sim_mujoco/data/in_place_long.csv\"\n",
    "    INPUT_FILE\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_columns = [\n",
    "    \"L_YAW_pos\", \"L_HAA_pos\", \"L_HFE_pos\", \"L_KFE_pos\", \"L_ANKLE_pos\",\n",
    "    \"R_YAW_pos\", \"R_HAA_pos\", \"R_HFE_pos\", \"R_KFE_pos\", \"R_ANKLE_pos\",\n",
    "    \"L_YAW_vel\", \"L_HAA_vel\", \"L_HFE_vel\", \"L_KFE_vel\", \"L_ANKLE_vel\",\n",
    "    \"R_YAW_vel\", \"R_HAA_vel\", \"R_HFE_vel\", \"R_KFE_vel\", \"R_ANKLE_vel\", \n",
    "    \"vel_x_BF\", \"vel_y_BF\", \"vel_z_BF\", \"normal_vec_x_BF\", \"normal_vec_y_BF\", \"normal_vec_z_BF\", \n",
    "    \"omega_x\", \"omega_y\", \"omega_z\", \"vx_des_BF\", \"vy_des_BF\", \n",
    "    \"right_foot_t_since_contact\", \"right_foot_t_since_no_contact\", \n",
    "    # \"right_foot_t_since_contact\", \n",
    "    \"right_foot_pos_x_BF\", \"right_foot_pos_y_BF\", \"right_foot_pos_z_BF\",\n",
    "    \"left_foot_t_since_contact\", \"left_foot_t_since_no_contact\",\n",
    "    # \"left_foot_t_since_contact\",\n",
    "    \"left_foot_pos_x_BF\", \"left_foot_pos_y_BF\", \"left_foot_pos_z_BF\"\n",
    "]\n",
    "\n",
    "action_columns = [\n",
    "    \"L_YAW_tau_ff\", \"L_HAA_tau_ff\", \"L_HFE_tau_ff\", \"L_KFE_tau_ff\", \"L_ANKLE_tau_ff\",\n",
    "    # \"L_YAW_tau_ff\", \"L_HFE_tau_ff\", \"L_KFE_tau_ff\", \"L_ANKLE_tau_ff\",\n",
    "    \"R_YAW_tau_ff\", \"R_HAA_tau_ff\", \"R_HFE_tau_ff\", \"R_KFE_tau_ff\", \"R_ANKLE_tau_ff\",\n",
    "    # \"R_YAW_tau_ff\", \"R_HFE_tau_ff\", \"R_KFE_tau_ff\", \"R_ANKLE_tau_ff\",\n",
    "    \"L_YAW_q_des\", \"L_HAA_q_des\", \"L_HFE_q_des\", \"L_KFE_q_des\", \"L_ANKLE_q_des\",\n",
    "    \"R_YAW_q_des\", \"R_HAA_q_des\", \"R_HFE_q_des\", \"R_KFE_q_des\", \"R_ANKLE_q_des\",\n",
    "    \"L_YAW_q_vel_des\", \"L_HAA_q_vel_des\", \"L_HFE_q_vel_des\", \"L_KFE_q_vel_des\", \"L_ANKLE_q_vel_des\",\n",
    "    \"R_YAW_q_vel_des\", \"R_HAA_q_vel_des\", \"R_HFE_q_vel_des\", \"R_KFE_q_vel_des\", \"R_ANKLE_q_vel_des\"\n",
    "]\n",
    "\n",
    "# we can also compare the saved NN output to the actual actions\n",
    "# but since we re-compute them in test_bc.ipynb, there is no need to\n",
    "# view it since that is using an old model.\n",
    "\n",
    "# EVENTUALLY, we would wanna do post-processing on the data to see how\n",
    "# well the model is doing during actual simulation. Then we can plot\n",
    "# the actions but eh\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "for dp in data_rel_paths:\n",
    "    ds = pd.read_csv(dp)\n",
    "    dataset = pd.concat((dataset, ds))\n",
    "num_steps = dataset.shape[0]\n",
    "states = dataset[state_columns].to_numpy(dtype=np.float64)\n",
    "actions = dataset[action_columns].to_numpy(dtype=np.float64)\n",
    "\n",
    "# remove the first 1000 data points, seems like noise (when the robot is floating down.)\n",
    "states = states[1000:, :]\n",
    "actions = actions[1000:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VIEW RAW INPUT STATES IN A LINE PLOT\n",
    "# okay now just try to scale the data\n",
    "def view_data(factor, data, data_columns):\n",
    "    plt.figure()\n",
    "    # create multiple subplots to compare the actions\n",
    "    for j in range(factor):\n",
    "        for ii in range(len(data_columns) // factor):\n",
    "            plt.subplot(1, 5, ii + 1)\n",
    "            index = (j * (len(data_columns) // factor)) + ii\n",
    "            plt.plot(data[:, index])\n",
    "            plt.title(data_columns[index])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def view_data2(data, data_columns):\n",
    "    # create singular plots for each state\n",
    "    for i in range(len(data_columns)):\n",
    "        plt.plot(data[:, i])\n",
    "        plt.title(data_columns[i])\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# view_data2(states[:, :], state_columns)\n",
    "\n",
    "# print(len(state_columns))\n",
    "# view_data(8, states[:, :], state_columns)\n",
    "\n",
    "# since it has 41 columns.\n",
    "# plt.plot(states[:, len(state_columns) - 1])\n",
    "# plt.title(state_columns[len(state_columns) - 1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- create multiple subplots to compare the actions\n",
    "# view_data(6, actions, action_columns)\n",
    "# view_data2(actions[1000:5000, :], action_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- See the distribution of the actions\n",
    "def plot_hist(data, data_columns):\n",
    "    for i in range(data.shape[1]):\n",
    "        plt.hist(data[:, i])\n",
    "        plt.title(data_columns[i])\n",
    "        plt.show()\n",
    "\n",
    "# plot_hist(states, state_columns)\n",
    "# plot_hist(actions, action_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- view the covariance matrix of the scaled states\n",
    "# scale the data then plot the heatmap\n",
    "# scaler_s = MinMaxScaler()\n",
    "scaler_s = QuantileTransformer()\n",
    "# fit the scaler, and then apply the transform right away\n",
    "scaled_states = scaler_s.fit_transform(states)\n",
    "\n",
    "# view_data(8, scaled_states, state_columns)\n",
    "\n",
    "df = pd.DataFrame(scaled_states, columns=state_columns)\n",
    "cov_matrix = df.cov()\n",
    "# sns.heatmap(cov_matrix)\n",
    " \n",
    "# --- view the covariance matrix of the actions, removing two columns\n",
    "# action_columns2 = [\n",
    "#     \"L_YAW_tau_ff\", \"L_HAA_tau_ff\", \"L_HFE_tau_ff\", \"L_KFE_tau_ff\", \"L_ANKLE_tau_ff\",\n",
    "#     \"R_YAW_tau_ff\", \"R_HAA_tau_ff\", \"R_HFE_tau_ff\", \"R_KFE_tau_ff\", \"R_ANKLE_tau_ff\",\n",
    "#     \"L_YAW_q_des\", \"L_HAA_q_des\", \"L_HFE_q_des\", \"L_KFE_q_des\", \"L_ANKLE_q_des\",\n",
    "#     \"R_YAW_q_des\", \"R_HAA_q_des\", \"R_HFE_q_des\", \"R_KFE_q_des\", \"R_ANKLE_q_des\",\n",
    "#     \"L_YAW_q_vel_des\", \"L_HAA_q_vel_des\", \"L_HFE_q_vel_des\", \"L_KFE_q_vel_des\", \"L_ANKLE_q_vel_des\",\n",
    "#     \"R_YAW_q_vel_des\", \"R_HAA_q_vel_des\",\n",
    "# ]\n",
    "# actions2 = dataset[action_columns2].to_numpy(dtype=np.float64)\n",
    "# df2 = pd.DataFrame(actions2, columns=action_columns2)\n",
    "# sns.heatmap(df2.cov())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_a = MinMaxScaler()\n",
    "scaler_a = QuantileTransformer()\n",
    "scaled_actions = scaler_a.fit_transform(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE NORMALIZED DATA\n",
    "# combine the scaled states and actions\n",
    "scaled_data = np.concatenate((scaled_states, scaled_actions), axis=1)\n",
    "# scaled_data = np.concatenate((scaled_states, actions), axis=1)\n",
    "\n",
    "# create a combined pandas dataframe\n",
    "df = pd.DataFrame(scaled_data, columns=state_columns + action_columns)\n",
    "\n",
    "# save the data\n",
    "df.to_csv(OUTPUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_data2(scaled_actions, action_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save the trained models:\n",
    "from pickle import dump\n",
    "\n",
    "# save the scalers\n",
    "with open(EXPORT_DATA_PATH + \"state_scaler_\" + EXPORT_VER + \".pkl\", \"wb\") as f:\n",
    "    dump(scaler_s, f, protocol=5)\n",
    "protocol=5\n",
    "with open(EXPORT_DATA_PATH + \"action_scaler_\" + EXPORT_VER + \".pkl\", \"wb\") as f:\n",
    "    dump(scaler_a, f, protocol=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
